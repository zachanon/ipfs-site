<!-- title: Knowledge Curation -->

### Thesis
- Our public knowledge curation tools are obsolete and hurting us. We must build new ones.

The day to day lives of the many billions of people connected in some way to the internet, and those that are merely in contact or affected by those that are, are influenced by the knowledge curation and dissemination tools that have been created.

These tools were built during a time period of extremely high bandwidth and data cost, and without the aid of modern information processing algorithms. With the capability we have now at our disposal, we can and should rebuild them. If we do not, the world will become a worse place for it.

### Evidence

- Gewgle censorship
- Wiki censorship
- Chyna censorship


- Why this is baked in



### What does New look like

- Algorithmic/NLP not human
- Composed from primary sources
- Composed from open sources
- Graph/Tree-like. The data is shared but the semantic content is interpreted differently across different platforms.

The new model is to be algorithmic, deterministic, and dynamic. Instead of layered human interpretation on top of primary sources, you get algorithmically mined representations of composed primary sources.  

The benefit of algorithmic knowledge curation on top of shared primary sources is the ability to *fork*. If you believe the algorithm is biased in its presentation of the facts, you can take the implementation and directly modify it, without starting from scratch.

### Conclusion/So What

- If we do not build new information/knowledge curation tools, we will be controlled by those that do.